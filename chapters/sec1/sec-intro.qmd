# DevOps Lessons for Data Science

DevOps grew out of the need for closer integration between the delivery of software and software services and the people writing that software. In the bad old days (and still some places), software was written by the dev team and then thrown over the wall to the ops team who would be responsible for releasing or managing that software.

Unsurprisingly, this division of labor resulted in really inefficient delivery of services. Things would break when they got thrown over the wall -- they'd be hosted badly or inefficiently, changes would take forever, and services were not production-ready.

So in many ways, DevOps is a component of the agile revolution in software engineering -- a recognition that the only way to reliably deliver great experiences is to integrate the development and operations process, resulting in the portmanteau DevOps. The whole goal of DevOps is to tighten feedback cycles between the people creating software and those responsible for releasing or hosting it.

To that end, there are many lessons Data Scientists can learn from DevOps practices and cultures.

In this section, I'm going to explore four ideas and one tool that are deeply integrated into DevOps practices and are highly applicable to Data Science as a practice. I'm not try to do an exhaustive survey of the DevOps landscape here.

For example, I'm not really going to talk about microservices -- an important component of DevOps, but one that I don't really see deep integrations for Data Science. But maybe I'm wrong and a future draft of this book will contain a section on microservices. It wouldn't shock me.

Either way, here are the topics this section will address.

## **Continuous Integration, Continuous Delivery/Deployment (CI/CD)**

As with all DevOps concepts, CI/CD is a combination of cultural and work practices, as well as tooling to support those practices.

CI/CD is the notion that it should be easy to develop new code and get it into an existing production app with little fuss. So CI/CD includes ideas about what are the scale and scope of changes that should be introduced to a system, how frequently they should be introduced, and the tooling that supports those changes.

In the data science world, the CI/CD practice often boils down to thinking about code promotion processes -- how does your code go from your laptop to production.

## Environments as Code

You may have heard that adage "cattle, not pets". This is the idea that anything that you deploy should be completely reproducible with some code, that you can have as many reproductions of it as you want that are functionally identical, and that without any bespoke nurturing you can diagnose a sick instance and replace it easily.

The solution to the "cattle, not pets" notion is Infrastructure as Code. Infrastructure as Code is both this idea of keeping everything you need to create, destroy, and replace your infrastructure in code. If you're a data scientist, you're probably not managing much infrastructure, but you might have reports, apps, and more. So infrastructure as code is about making each piece of content standalone and able to reproduce easily.

In this section, we'll talk about some best practices that you probably need to know, as well as the biggest and hardest topic for cattle not pets for data scientists, managing your package environment.

## Data Science Microservices

Separate frontend from backend.

Separate app logic from data updates.

## Monitoring + Logging

You need it. I'm not sure I have a lot of interest to say here...
