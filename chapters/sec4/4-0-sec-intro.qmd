# Making it Enterprise-Grade {#sec-3-intro}

Over the last few chapters, we walked through getting set up with a
server, accessing and managing the server via SSH, understanding DNS and
getting a real URL, securing the server with SSL/HTTPS, and right-sizing
the server for your needs.

And if you walked through the process with the labs, you've got a data
science workbench running RStudio Server, JupyterHub, and a
model-serving API all of your own. That's awesome!

That server is great if you're working alone on some data science
projects you want to host in the cloud or if you've got a small team of
data scientists in a small organization.

But if you work at a large organization, an organization with
more-stringent security needs, or a larger team of data scientists,
you're going to need to start considering some more complex ways of
managing your data science environment for security and stability.

Broadly speaking, these concerns fall under the context of *enterprise*.
In this context, Enterprise roughly translates to "a large organization
with well-defined IT/Admin responsibilities and roles". If you're not in
an Enterprise, you likely have the rights and ability to just stand up a
data science workbench and use it, like we did in the last section.

If you are in an Enterprise, you probably had to do all that on a
personal account, and almost certainly couldn't connect to your real
data sources.

Doing open source data science in an Enterprise almost certainly means
having to work across teams to get your workbench built and having to
convince other people that you'll be a good actor at the end of the day.

The goal of the next few chapters is to help you understand the ways
your data science workbench isn't enterprise and how to communicate with
the IT/Admins at your organization who are responsible for such things.
We'll get into how to integrate open-source data science into an
organization, what more complex network architecture might look like,
how auth works, and how to scale your environment.

Hopefully you won't have to implement much of this yourself. Instead,
the hope is that reading and understanding the content in this chapter
will help make you a better partner to the teams at your organization
who *are* responsible for these things. You'll be equipped with the
language and mental models to ask good questions and give informative
answers to the questions the IT/Admins have about your team's
requirements.

## The deal with security

As a data scientist, you probably have a getting-things-done kind of
mindset. And that's great!

But if you're having to interact with security professionals at your
organization, you're probably finding that they have a whole different
set of concerns -- perhaps concerns that you're struggling to
communicate with them about.

So maybe you're frustrated to notice that there *isn't* a chapter in
this section on security. That's because security isn't just one
particular set of practices or decision. Instead, security is about
negotiating tradeoffs and making the best decision for your
organization.

Indeed, this entire part of the book is really about security and the
closely-related topic of stability.

Broadly, security for a data science platform is about ensuring its
usefulness. And it's true that a data science platform that allows bad
actors to break in and steal data is not useful. But it's also the case
that a platform that doesn't function because its too locked down or
because someone used all the system resources and crashed the server
also is not useful.

A great security team is constantly considering tradeoffs regarding
*access* and *availability*.

Access is about making sure that the right people can interact with the
systems they're supposed to and that unauthorized people can't.
Availability is about making sure that your enterprise-grade systems are
around when people need them, and that they are stable during whatever
load they face during the course of operating.

Lest you think you're immune because you're not an interesting target,
there are plenty of bots out there randomly trying to break in to every
existing IP address, not because they care about what's inside, but
because they want to co-opt your resources for their own purposes like
crypto mining or virtual DDOS attacks on Turkish
banks.[^4-0-sec-intro-1]

[^4-0-sec-intro-1]: Yes, these both really happened.

Moreover, security and IT professionals aren't just concerned with bad
actors from outside (called outsider threat) or even someone internal
who decides to steal data or resources (insider threat). They are (or at
least should be) also concerned with accidents and mistakes -- these are
just as big a threat to accessibility and availability.

For example, many basic data science workbenches give root access to
everyone -- it's easy and simple! That obviously exposes the possibility
that someone with access to your server could decide to steal data for
their own ends (insider threat). But it also opens up the possibility
that someone makes a mistake! What if they mean to just delete a project
directory, but forget to add the whole path in and wipe your whole
server. Yikes!

Stability is also a big concern that rises along with the scale of the
team and the centrality of their operations to the functioning of your
organization.

If you're a small data science team, you might not be too concerned if
someone accidentally knocks your data science workbench offline for 30
minutes because they tried to run a job that was too big. You're
probably all sitting in the same room and you can learn something from
the experience.

That's not the case when you get to enterprise-grade tooling. An
enterprise-grade data science workbench probably supports dozens or
hundreds of professionals across multiple teams. The server being down
isn't a sorta funny occurrence you can all fix together -- it's a
problem that must be fixed immediately -- or even better avoided
altogether.

IT/Admins think hard about how to provide resources in a way that avoids
having servers go down because they're hitting resource constraints.

Back in section one, we learned about environments as code -- using code
to make sure that our data science environments are reproducible and can
be re-created as needed.

This idea isn't original -- in fact, DevOps has it's own set of
practices and tooling around using code to manage DevOps tasks, broadly
called Infrastructure As Code (IaC). This chapter will get broadly into
some of the things you can do with IaC tooling, and will introduce some
key concepts and some of the popular tooling options for doing IaC you
can try out.

There is no one-size-fits-all (or even most) position for security.
Instead, great security teams are constantly articulating and making
decisions about tradeoffs.

