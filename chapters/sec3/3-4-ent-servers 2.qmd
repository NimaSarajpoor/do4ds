# Compute at enterprise scale {#sec-ent-scale}

Managing enterprise-scale computational power requires an approach to
match the size of the problem. Patterns that work fine in a smaller
environment start to break down in an environment that supports 100 or
500 or even 1,000 data scientists working simultaneously.

That many users and use cases requires multiple -- perhaps many --
servers. With so many users, downtime becomes very expensive, the
stability of the system becomes paramount, and cost management becomes
more important.

This chapter will help you understand how enterprise IT/Admins think
about managing compute at enterprise scale and how to communicate with
them about your needs as a data scientist.

## Managing many servers

In an enterprise, the IT/Admin group is managing dozens or hundreds or
thousands of servers, which requires thinking about them differently.
Increasingly, enterprise organizations are moving away from managing
servers at all, acquiring PaaS or SaaS software from cloud providers so
they don't ever deal with server migrations or operating system
upgrades.

For organizations that do still manage their own servers, they're very
focused on keeping them synchronized and avoiding *state drift*. State
drift is what happens when you let people `ssh` into a server and make
an adjustment to this setting here and isntall that system library
there. This idea is often encompassed in the adage that "servers should
be cattle, not pets".

When you're standing up an environment, you first need to *provision*
the environment, by standing up the servers and the networking between
them. Once the servers are stood up, they need to be *configured* to do
what you want to do. Configuration includes installing applications like
Python, R, JupyterHub, RStudio Server, or whatever else goes on the
server. In many enterprise organizations, provisioning and configuration
are actually done by separate groups. They may be called the server
group and the application administration group.

Like you want a Dev/Test/Prod setup for your data science projects as
discussed in [Chapter @sec-deployments], IT/Admins usually use a
Dev/Test/Prod setup for themselves. They want an environment to test
changes to the servers and applications themselves.

For enterprises, I recommend a two-dimensional Dev/Test/Prod setup where
the IT/Admin group make changes to the data science environment itself
in *staging*.[^3-4-ent-servers-1] Data scientists never get access to
the staging environments and do all their work in the IT/Admin prod
environment.

[^3-4-ent-servers-1]: I call it staging because I call the data science
    environments Dev/Test/Prod. You'll have to work out language with
    your IT/Admin group.

Since the point is to keep the prod-prod version of a project completely
pristine, data scientists may promote their own work into prod-prod, or
it may require an IT/Admin to do that.

![](images/dev-test-prod.png){fig-alt="The IT/Admin promotes the complete staging environment, then the data scientist or IT/Admin promote within Prod."
width="600"}

Changes that require moves from staging to prod, including upgrades to
applications, adding system libraries, or operating system upgrades
often have rules around them. They may need to be validated or approved
by security. In some highly-regulated environments, the IT/Admin group
may only be able to make changes during certain windows of time. This
can be a source of tension between a data scientist who needs a new
library or version now and an IT/Admin who isn't allowed to move fast.

In addition to changes that go from staging to prod, enterprises also
sometimes undergo a complete rebuild These days, many of those rebuilds
are the result of a move to the cloud. Planning for a cloud migration
can be a multi-year affair.

In order to achieve this kind of replace-ability, many IT/Admin
organizations use *Infrastructure as Code (IaC)* tools to manage the
servers inside their environment.[^3-4-ent-servers-2] There are many
different IaC tools used by different organizations including Terraform,
Ansible, CloudFormation (AWS's IaC tool), Chef, Puppet, and Pulumi.

[^3-4-ent-servers-2]: Using IaC isn't at all particular to enterprises.
    Every good IT/Admin group uses IaC, but it becomes especially
    important in enterprise settings.

::: callout-note
### No Code IaC

Some enterprises manage servers without IaC. These usually involve
writing extensive *run books* to tell another person how to configure
the servers. If your spidey sense is tingling that this probably isn't
nearly as good as code, you're right. Enterprise IT/Admin organizations
that don't use IaC tooling is definitely a red flag.
:::

Most IaC tools can do both provisioning and configuration to some
degree, but most specialize in one or the other, so many organizations
use a pair of tools. One alternative to doing live configuration on a VM
is to provision the machine and then just launch a pre-built AMI or
Docker Container on the VM to shortcut the need to install software
right on the machine.

Some organizations also only put certain parts of the process in IaC.
For example, if you actually wanted to create an entire data science
environment using IaC in AWS, you would need to include standing up a
VPC or create a new K8S cluster. But that's really rare, so a lot of IaC
assumes that the environment has already been stood up to a certain
extent.

## Bringing users together

At some scale, you outstrip the ability of any one server -- even a big
one -- to accommodate all of the data science work that needs to get
done. The moment depends a lot on the computational needs of the actual
work. If you're doing highly intensive simulation work or deep learning,
you may hit it with one data scientist. I've occasionally seen single
servers that comfortably fit 50 concurrent users because they were only
doing light computational tasks.

As you add more people, you also are likely to add more variety in
requirements. For example, they might want to be able to incorporate
several different sizes of jobs that might require a few different sized
nodes. Or maybe they want to run a mixture of GPU-backed and non-GPU
workloads in the cluster and want to reserve GPU nodes for the jobs that
need it.

Once you've hit that point, you've got to *horizontally scale*. Now,
there's the simple way to horizontally scale, which is just to give
every user or every group their own disconnected server to use. In some
organizations with groups that are really disconnected one from another,
this can work very well. The downside is that there usually needs to be
some sort of requisition system to get a server. And the IT/Admin is
still left managing a bunch of different servers, each of which might
look special.

In enterprises, this usually isn't the way things get done. Most
enterprises want to run one centralized service that everyone in the
company -- or at least across a large group -- come to. Managing just
one environment makes things simpler, but in an environment with so many
users, downtime is very expensive. An hour of downtime for 500 data
scientists wastes \$25,000.[^3-4-ent-servers-3]

[^3-4-ent-servers-3]: Assuming a (probably too low), fully-loaded cost
    of \$100,000 and 2,000 working hours per year.

::: callout-note
### Measuring Uptime

Downtime limits are often measured in *nines of uptime*. This refers to
the proportion of the time that the service is guaranteed to be online
out of a year. So a one-nine service is guaranteed to be up 90% of the
time, allowing for 36 days of downtime a year. A five-nine service is up
for 99.999% of the time, allowing for only about 5 1/4 minutes of
downtime a year.
:::

That's why enterprise organizations take avoiding downtime very
seriously. Aside from following good DevOps practices like using IaC and
testing changes in a staging environment, many organizations take
further steps to limit downtime.

Most enterprise organizations retain some servers for *disaster
recovery*. This means different things at different organizations. For
some, they maintain snapshots of the state of the server (often nightly)
so the IT/Admins can just roll back to a previous known good state in
the event of a failure. Sometimes it also means that there is actually a
copy of the server waiting on standby to be activated at all times.

Other times, there are stiffer requirements such that nodes in the
cluster could fail and the users wouldn't be meaningfully affected. This
requirement for limited cluster downtime is often called *high
availability*. High availability is not a description of a particular
technology or technical approach -- though it is sometimes treated as
such. High availability is a description of a desired outcome for a
cluster and different organizations have different definitions.

## Computing in clusters

Whether it's for horizontal scaling or high availability reasons, most
enterprises run their data science environments in a *cluster*. The goal
is that a cluster still feels like working in a single server
environment, but on the backend there are multiple -- perhaps many --
servers to add computational power, provide different kinds of
computational environments, and/or to add resilience if one server were
to fail.

Running in a cluster adds two sources of complexity. First, how do users
know which node to go to? Second, once they get there, how do you make
sure everything they need is available. After all, the whole point of
the cluster is defeated if I can only ever use one node because that's
where my code and data are and where my packages are installed.

The where do I go problem is solved by providing a single front door
using a technology called a *load-balancer*, which is a kind of proxy
server. The load-balancer sits in front of all the nodes to the cluster,
providing a common front door and routing users to an appropriate node.

On the backend, the IT/Admin will set up the nodes so that all of your
data doesn't actually live on any of the nodes. Instead, they set up
separate storage that houses everything you need to actually work. That
way each of the nodes can symmetrically access your code and
data.[^3-4-ent-servers-4]

[^3-4-ent-servers-4]: There is often also data that the data science
    hosting software itself needs to store, like your user preferences.

TODO: add cluster label to image

![](images/lb-cluster.png){fig-alt="Users come to the load balancer, which sends them to the nodes, which connect to the state."}

If you are a solo data scientist reading this -- please do not try to
run your own horizontally-scaled data science cluster. When you
undertake load balancing, you've taken on a distributed systems problem
and those are **inherently difficult**.

If your organization has high availability requirements, it's worth
considering that just adding more nodes may not be sufficient. As the
requirements for high availability get steeper, the engineering cost to
make sure the service really is that resilient rise exponentially. In
addition to considering what to do if a node goes offline, you also have
to have backup for the load-balancer and the database/NAS, as well as
any other part of the system.

::: callout-note
For technical information on how load-balancers work, see [Appendix
@sec-append-lb].
:::

In fact, it's totally possible to make your system *less stable* by just
doing horizontal scaling in one spot without thinking through the
implications.

## Docker in enterprise = Kubernetes

Originally created at Google and released in 2014, the open source
*Kubernetes (K8S, pronounced koo-ber-net-ees or kates for the
abbreviation)* is the way to run production services out of Docker
containers.[^3-4-ent-servers-5] Many organizations are moving towards
running much or all of their production work in Kubernetes.

[^3-4-ent-servers-5]: If you are pedantic, there are other tools for
    deploying Docker containers like Docker Swarm and Kubernetes is not
    limited to Docker containers. But those are corner cases.

::: callout-note
Apparently Kubernetes is an ancient Greek word for "helmsman".
:::

Relative to managing individual nodes in a load-balanced cluster,
Kubernetes makes things way easier because it completely separates
provisioning and configuration. In Kubernetes, you create a cluster of a
certain number of nodes. Then, you put your applications into Docker
Containers and add them to the Kubernetes cluster. A container running
in Kubernetes is called a *pod*.

The elegance of Kubernetes is that you don't have to think at all about
where each pod goes. Instead, the Kubernetes engine *schedules* the pods
on the nodes to efficiently use the nodes you've
provided.[^3-4-ent-servers-6]

[^3-4-ent-servers-6]: When you request a pod from the Kubernetes engine,
    you specify how much resources you need for the pod. You limits and
    requests.

TODO: Graphic of Kubernetes

From the perspective of the admin, this is extremely powerful because
you just make sure you've got enough horsepower in the cluster and then
all the app-level requirements go in the container. Then when you
declare how many pods you want, you don't have to worry about what's
going on with each of the nodes in the cluster because it is just
running Kubernetes and Docker.

These days, almost everyone is using a cloud provider's managed
Kubernetes service -- AWS's *EKS (Elastic Kubernetes Service*, Azure's
*AKS (Azure Kubernetes Service)*, or GCP's *GKE (Google Kubernetes
Engine)*.[^3-4-ent-servers-7]

[^3-4-ent-servers-7]: It's rare, but some organizations do run an
    on-prem Kubernetes cluster with Oracle's OpenShift.

One really nice thing about using these Kubernetes clusters as a service
is that adding more nodes is just a few clicks away. On the other hand,
that also makes it dangerous from a cost perspective.

For production purposes, the applications in Kubernetes clusters are
usually managed with *Helm charts*, which are the standard IaC way to
declare what pods, how many of each, and their relationship inside the
cluster.

Kubernetes is an extremely powerful tool and there are many reasons to
like it. That said, it's also a complicated tool and becoming a
proficient Kubernetes admin is a skill unto itself. These days, many
IT/Admins are trying to add Kubernetes to their list of skills. You
should be careful that your data science environment doesn't become
someone's first foray into Kubernetes.

In the extreme version, some enterprises are going to "we just run a
single kubernetes cluster for our entire organzation". In my opinion,
this will someday be a great pattern, but it's still a little immature.
In particular, Kubernetes kinda makes the underlying assumption that all
of the resources for the cluster are shared across the cluster. There
are ways to use namespaces and other things -- but not all Kubernetes
resources can be split into namespaces, and managing a Kubernetes
cluster for multiple use cases or groups isn't at all trivial.

## Supporting variable use cases

It's easiest to create a cluster where you just have a bunch of
identical servers sitting next to each other. But that's often not what
you need in an enterprise context. Instead, you may have some workloads
that need only a small amount of compute and others that need a very
large server. You may have some workloads that need GPU-based instances,
which are too expensive for most organizations to want to keep online
all the time.

By far the simplest way to manage this complexity is to run a "main"
cluster for everyday kind of workloads and stand up additional special
environments with extra large servers or GPU-backed instances as needed.

In general, it's not trivial to set up a single cluster that can support
different kinds of workloads, and it's often easier to set up, for
example, a temporary standalone GPU cluster.

It is possible to run one cluster that supports multiple different kinds
of nodes behind the scenes. Some organizations turn to Kubernetes for
this kind of workload (more on that below). Often, a better option for
data science workloads is to use use a high-performance computing (HPC)
framework. HPC is particularly appropriate when you need very large
machines. In Kubernetes, it is not possible to schedule pods larger than
the size of the actual nodes in the cluster. In contrast, most HPC
frameworks allow you to combine an arbitrary number of nodes into what
acts like a single machine with thousands or tens of thousands of nodes.

For example, Slurm is an HPC framework that support multiple queues for
different sets of machines. AWS has a service called ParallelCluster
that allows users to easily set up a Slurm cluster -- and with no
additional cost relative to the cost of the underlying AWS hardware.

Another solution people sometimes try to undertake is to implement
autoscaling. The idea here is that the organization could maintain a
small amount of "always-on" capacity and scale out other capacity as
needed to maintain costs. This is possible -- but it requires nontrivial
engineering work.

In particular, autoscaling a data science workbench down is quite hard.
The main reason for this is that many autoscaling routines assume you
can easily move someone's job from one node to another just keeping
track of the long-term state. This is a bad assumption for a data
science workbench and autoscaling a data science workbench downwards is
a difficult challenge.

Let's think about a relatively stateless workload. For example, let's
imagine a search engine -- every time you put in a search, it spins up a
job, does your search, and then spins down.

If you come back in a minute or two, it can just spin up another job. It
only needs to remember your last query, but that's a pretty simple bit
to state to pass around. If you're on a different node, no big deal!

It's easy to look at the power of Kubernetes and think it will make
everything easy. This is not the case. While the high-level outlines of
Kubernetes are super appealing, it is still a complex tool and a data
science workbench is a particularly difficult fit for Kubernetes.

In many organizations, adopting Kubernetes is synonymous with trying to
do autoscaling. As previously discussed, autoscaling a data science
workbench is a particularly difficult task -- and Kubernetes is
particularly friendly to stateless workloads. Autoscaling in Kubernetes
with a data science workbench really requires a highly competent
Kubernetes admin.

That's not the case with a data science workbench. Many autoscaling
frameworks these days assume that applications are mostly stateless. In
a stateless applications, every interaction is standalone -- there's
very little path dependence. The line between statefulness and
statelessness are pretty blurry, but working inside a development
environment like RStudio or a Jupyter Notebook is about as stateful as
it gets. You have long-term state like the files you need and your
preferences and short-term state like the recent commands you've typed
and the packages loaded into your R and Python environment.

## Comprehension Questions

1.  What is the difference between horizontal and vertical scaling? For
    each of the following examples, which one would be more appropriate?
    a.  You're the only person using your data science workbench and run
        out of RAM because you're working with very large data sets in
        memory.

    b.  Your company doubles the size of the team that will be working
        in your data science workbench. Each person will be working with
        reasonably small data, but there's going to be a lot more of
        them.

    c.  You have a big modeling project that's too large for your
        existing machine. The modeling you're doing is highly
        parallelizable.
2.  What is the role of the load balancer in horizontal scaling? When do
    you really need a load balancer and when can you go without?
3.  What are the biggest strengths of Kubernetes as a scaling tool? What
    are some drawbacks?
